{"meta":{"title":"苗春童的博客","subtitle":"","description":"","author":"苗春童","url":"https://shijiazhuangbaifeng.github.io","root":"/"},"pages":[{"title":"分类","date":"2020-07-08T02:16:20.000Z","updated":"2020-07-08T02:18:44.079Z","comments":true,"path":"categories/index.html","permalink":"https://shijiazhuangbaifeng.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-05-27T05:47:40.000Z","updated":"2020-07-08T02:18:37.597Z","comments":true,"path":"tags/index.html","permalink":"https://shijiazhuangbaifeng.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"JVM","slug":"JVM","date":"2020-08-01T02:53:21.000Z","updated":"2020-08-01T06:18:37.795Z","comments":true,"path":"2020/08/01/JVM/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/08/01/JVM/","excerpt":"","text":"什么是JVM定义jvm 是运行 java 二进制字节码运行的环境好处：JVM 是 java 程序实现一次编写到处运行的重要条件自动内存管理，自动回收数组下标越界检查比较比较 JRE JVM JDK 三者之间的关系：学习JVM是中高级程序员的必备技能内存结构程序计数器定义Program Counter Register 程序计数器（寄存器）作用在程序执行的过程中，记录下一条 JVM 指令执行的地址特点线程私有，每一个线程都有自己的程序计数器。唯一一个不会出现内存溢出的部分栈定义栈（Stacks）的数据结构类似于弹夹，先进后出的结构，栈的每一个元素称之为栈帧，一个栈帧就是一个方法的调用，栈帧是每个方法调用所需要的内存。方法执行完，栈帧就会被释放，每个线程都有一个活动栈帧，对应着正在执行的方法。问题垃圾回收会不会管理栈？不会，因为当一次方法执行结束，栈帧的数据会自动销毁栈内存分配越大越好吗？Linux 和 macOS 的默认内存为 1024kb，可以使用 -Xss1m 来配置内存，这里配置的是单个栈的内存，单个栈内存分配的大，会影响线程的数量。方法内的局部变量是否线程安全？这要看这个变量是不是线程共享的栈内存溢出导致栈内存溢出的原因有：栈帧过多，例如递归没有设计好栈帧过大，一个栈帧的元素查过了栈的内存异常信息为：java.lang.StackOverflowError本地方法栈本地方法栈 (Native Method Stacks)，本地方法指的是不是由JAVA编写的方法，本地方法栈就是为本地方法的运行提供内存空间的。1public final native Class&lt;?&gt; getClass();堆程序计数器、栈和本地方法栈都是线程私有的，而本地方法栈则是线程共享的，一个JVM只会有一个堆空间，通过 new 关键字创建的对象都会创建在堆当中。可以通过 -Xms1024m 设置内存大小。堆内存溢出堆内部虽然有垃圾回收机制，但是回收的对象是无用的对象，当创建的对象过多的时候也会内存溢出：Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space。堆内存诊断jps 查看系统中有哪些JVM进程jmap 查看堆内存的占用情况jconsole 图形化界面，多功能的检测工具123456789101112131415public class HeapDome01 &#123; public static void main(String[] args) throws InterruptedException &#123; System.out.println(\"1.....\"); Thread.sleep(3000); byte[] bytes = new byte[1024 * 1024 * 10]; System.out.println(\"2.....\"); Thread.sleep(3000); bytes = null; System.gc(); System.out.println(\"3.....\"); Thread.sleep(1000000000l); &#125;&#125;使用 jmap 查看内存占用情况：1jmap -heap方法区方法区是所有虚拟机线程共享的部分，存储跟类结构相关的信息：方法、构造器、常量，在虚拟机启动的时候被创建，逻辑上是属于堆的一部分。1.8 以后方法区是有元空间进行实现，放在了虚拟内存当中，内存有多大，元空间就有多大，可以通过：-XX:MaxMetaspaceSize=8m来进行设置直接内存常见于 NIO 操作，用于数据缓冲区，分配回收成本高，单读写性能高，并且不受JVM虚拟机管理。基本使用","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://shijiazhuangbaifeng.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://shijiazhuangbaifeng.github.io/tags/JAVA/"},{"name":"原理","slug":"原理","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"代理池","slug":"代理池","date":"2020-08-01T02:53:21.000Z","updated":"2020-08-01T06:28:50.527Z","comments":true,"path":"2020/08/01/代理池/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/08/01/%E4%BB%A3%E7%90%86%E6%B1%A0/","excerpt":"","text":"代理池概述什么是代理池代理池就是由多个代理IP组成的池子，他可以提供很多高质量的可用代理 IP。为什么要使用代理池最常见的反爬虫手段就是当一个 IP 频繁的访问，就会将这个 IP 拉入黑名单，解决这个问题的手段就是代理IP当一个代理IP封掉以后，换其他的代理IP。代理池的设计代理池的工作流程代理池的工作流程图如下：工作流程如下：从代理网站上获取代理IP信息 —&gt; 检测代理IP —&gt; 返回可用的代理IP —&gt; 写入到数据库中代理IP检测模块 —&gt; 从数据库中获取所有代理IP —&gt; 检测代理IP —&gt; 如果不可使用分数-1，分数为 0 删除IP，可以使用分数设置为默认IP，跟新数据库爬虫想proxy_api 所有代理API —&gt; 查询数据库代理池模块代理池主要分为五大模块：爬虫模块：采集代理 IP从代理IP网站上采集代理IP校验模块：校验IP可用性检验IP 性能，响应速度数据库模块：对数据库进行增删改查操作检测模块：获取数据库中代理IP，进行处理，保证代理IP的可用性API 模块：爬虫用来获取可用的IP","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"代理","slug":"代理","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E4%BB%A3%E7%90%86/"},{"name":"爬虫","slug":"爬虫","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"爬虫","slug":"爬虫","date":"2020-08-01T02:53:21.000Z","updated":"2020-08-01T06:18:37.744Z","comments":true,"path":"2020/08/01/爬虫/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/08/01/%E7%88%AC%E8%99%AB/","excerpt":"","text":"CMD 命令使用curlcurl 指定一个 URL 路径，访问，并返回网页的内容：1curl www.baidu.com参数作用示例-A设置 user-agentcurl -A &quot;chrome&quot; www.baidu.com-X设置请求的方式curl -X POST www.baidu.com-I只返回请求的头信息curl -I www.baidu.com-dPOST请求携带参数curl -d &quot;username=12345&amp;password=122&quot; 127.0.0.1/post-O下载文件并以远程的文件名保存curl -O http://127.0.0.1/image/jpeg-o自定义文件名保存curl -o finename http://127.0.0.1/image/jpeg-L可以重定向curl -L -I https://www.baidu.com-H设置请求头curl -H &quot;accept:image/jpeg&quot;-k允许发送一个不安全的SSL请求-D发起一个带 cookie的请求-s不显示其他无关信息wget下载一个文件：1wget www.baidu.comPython 库urlliburllib 是比较基础的爬虫库，Python 内置的库，urllib 中有三个模块，分别是：requestresponseparse使用 urllib导入 urllib.request 部分：123import urllib.requestr = urllib.request.urlopen('http://192.168.159.128/get')urllib.request.urlopen 是对原始操作的封装，他的参数有： url 指定所需打开的网址，这个方法返回的是 HttpResponse 对象。1234567# 从缓存中读取,并解码text = r.read().decode()# 转换为字典data = json.loads(text)print(text)由于从 HttpResponse 读取到的数据是字节，需要先对数据进行解码，如果数据是 json 数据还可以进行转换，HttpResponse 的对象还可以获得状态码Basic Auth在登录方式有一种登录方式为 Basic Auth 是由浏览器来提供登录框：urllib 对此提供了解决方案：123456789101112131415161718import urllib.request as requestimport json# 创建密码管理器realm = request.HTTPPasswordMgrWithDefaultRealm()# 添加账号密码realm.add_password(realm=None, uri='http://192.168.159.128/basic-auth/zhangsan/123456', user=\"zhangsan\", passwd=\"123456\")auth_handler = request.HTTPBasicAuthHandler(password_mgr=realm)opener = request.build_opener(auth_handler)request.install_opener(opener)result = request.urlopen(url='http://192.168.159.128/basic-auth/zhangsan/123456')print(result.read().decode())requestsrequests 是最常用的 python 常用第三方库，他比 urllib 更加的好用，他对 urllib 进行了一次封装：1pip install requests使用 request12345678910import requests# GET 请求result = requests.get('http://192.168.159.128/get')print(result.status_code, result.headers)print(result.json())# POST 请求result = requests.post('http://192.168.159.128/post')print(result.content)返回的对象如果是 json 数据，可以使用 json 方法直接转换为字典。抛出异常当状态码为：400 ~ 600 之间的时候，抛出异常，方便查找错误：1result.raise_for_status()Session123456789101112import requests# 创建 session 对象session = requests.Session()# get 返回数据result = session.get('http://192.168.159.128/cookies/set/username/zhangsan')# 下一次请求的时候会将当前的 session 的域名相关的 cookie 带上cookies = session.get('http://192.168.159.128/cookies')print(cookies.json())bs4bs4 是对 html 结构的内容进行处理，使用面向对象的方式来操作 HTML 内容，安装 bs4 ：1pip install bs4使用 bs4123456789import requestsimport bs4from bs4 import BeautifulSoupresult = requests.get('http://www.baidu.com')html_content = BeautifulSoup(result.content.decode())print(html_content.title)title 是一个对象，使用 get_text 方法就能够获得标签的内容。BeautifulSoup 常用成员方法 / 属性作用find_all(tag)根据 tag 标签名称获取 ，所有子节点children获取所有的子标签select(class_name)根据查询符合条件的标签，格式和 css 的格式一样attrs获取当前标签的属性，格式是字典lxmllxml是python的一个解析库,支持HTML和XML的解析,支持XPath解析方式,而且解析效率非常高XPath ：1pip install lxml和 bs4 整合1234567891011import requestsimport bs4from bs4 import BeautifulSoupresult = requests.get('http://www.baidu.com')html_content = BeautifulSoup(result.content.decode(), 'lxml')print('* ' * 20)print(html_content.text)单独使用1234567891011121314import requestsfrom lxml import etree# 获取数据result = requests.get('http://www.huanyue123.com/book/64/64118/').content.decode('gbk')# 创建选择器select = etree.HTML(result)# 页面所有的连接links = select.xpath('//li/a/@href')for link in links: print(link)XPATH在上面的案例中使用到了 XPATH ，XPATH 不是 python 的库，而是一门语言，使用 XPATH 能够是很方便的查询。ScrapyScrapy 是一个为了爬取网站数据，提取结构性数据的而编写的应用程序，可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。安装 Scrapy ：1pip install scrapy使用 Scrapy12345678910111213141516171819202122232425import scrapyclass QuotesSpider(scrapy.Spider): # 爬虫名称 name = 'quotes' # 目标网站 start_urls = ['http://quotes.toscrape.com/',] # 业务方法 def parse(self, response): # 获取信息 names = response.xpath('//div[@class=\"quote\"]//small/text()') contents = response.xpath('//div[@class=\"quote\"]/span[@class=\"text\"]/text()') for name, content in zip(names, contents): yield &#123; 'author': name.extract(), 'content': content.extract() &#125; next_link = response.xpath('//nav/ul/li[@class=\"next\"]/a/@href').extract_first() if next_link: print(next_link) yield response.follow(next_link, self.parse)运行这个 python 文件：1scrapy runspider quotes_scrapy.py-o ：将内容写入到一个文件中-t：指定文件格式Scrapy 入门使用命令，创建一个 Scrapy 项目：1scrapy startproject Day01该命令会创建以下的目录结构：123456789Day01&#x2F; scrapy.cfg Day01&#x2F; __init__.py items.py pipelines.py settings.py spiders&#x2F; __init__.py输入以下命令：1scrapy genspider quotes http://quotes.toscrape.com/在 Day01/Day01/spiders 目录下会增加一个 quotes.py 文件：1234567891011# -*- coding: utf-8 -*-import scrapyclass QuotesSpider(scrapy.Spider): name = 'quotes' allowed_domains = ['http://quotes.toscrape.com/'] start_urls = ['http://http://quotes.toscrape.com//'] def parse(self, response): passname：爬虫的名称allowed_domains： 所爬网页的域名必须要一致start_url : 爬虫网页入口在 parse 方法中填写业务逻辑以后，执行：1scrapy crawl quotesHtmlResponseparse 的 response 方法参数的数据类型是：HtmlResponse，他的常用方法有：属性/方法作用xpath(query)query ，支持 xpath 的方式获取数据，返回 selector 节点列表css(query)支持通过使用 css 的过滤数据，返回 selector 节点列表extract()序列化该节点为 unicode 字符串，返回字符列表extract()序列化该节点为 unicode 字符串，返回列表的第一个元素re(query)根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。以上方法，对于 selector 对象同样适用。12345678910111213# -*- coding: utf-8 -*-import scrapyclass QuotesSpider(scrapy.Spider): name = 'quotes' allowed_domains = ['http://quotes.toscrape.com/'] start_urls = ['http://http://quotes.toscrape.com//'] def parse(self, response): .... yield response.fllow(url, callback) ....response.fllow 的两个参数的作用为：url ：前往的urlcallback：回调函数，默认是 parse调试爬虫1234# 进入控制台，适用的是控制台环境scrapy shell # 进入控制台，带一个 url 参数scrapy shell url进入控制台以后，可以使用以下函数和对象：函数作用fetch(url)请求 url 或 request 对象，使用后会刷新 request 和 response 对象view()使用浏览器打开 response 的网页路径shelp()打印帮助信息spider相关 Spider 类的实例sttings保存所有配置信息的对象pipelinespipelines 的作用相当于是 AOP 切面，编写在 pipelines.py 文件中：1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlclass Day01Pipeline: def open_spider(self, spider): \"\"\" 初始化 :return: \"\"\" pass def process_item(self, item, spider): \"\"\" 每产生一个 item 对象，就会执行一次方法 :param item: :param spider: :return: \"\"\" return item def close_spider(self, spider): \"\"\" 初始化 :return: \"\"\" pass创建好以后，需要在 settings 文件中配置：123ITEM_PIPELINES = &#123; 'Day01.pipelines.Day01Pipeline': 300,&#125;ITEM_PIPELINES 的 key 是 Day01Pipeline 的全路径 ,value 是优先级，判断是哪一个先执行，process_item 是必须的并且必须返回一个 item 对象，如果不返回就不会执行下一个 process_item。如果想要将 item 抛弃掉，需要抛出一个异常：1reise DropItem()中间件中间件定义在 middlewares 文件中，中间件的方法主要有：process_request在 request 传往 downloader 的过程中和将下载结果返回给 engine 的过程中被调用，他的返回值如下，根据返回值的不同会有不同的行为:返回行为None一切执行正常，继续执行下一个中间件链Response停止调用 process_request 和 process_exception 方法，也不再继续执行下一个中间件，去执行process_responseRequest不在调用其他的 process_request 方法，由调度器从新安排下载IgnoreRequestprocess_exception 会被调用，如果没有此方法，则 request.errback会被调用，此方法也没有，该方法将会被彻底忽略process_response将下载结果返回给 engine 的过程中被调用：返回行为Response继续调用其他中间件的 process_response 方法Request不在调用其他的 process_request 方法，由调度器从新安排下载IgnoreRequestprocess_exception 会被调用，如果没有此方法，则 request.errback会被调用，此方法也没有，该方法将会被彻底忽略process_exception下载的过程中发生异常或者其他方法返回 IgnoreRequest 的时候会被执行：返回行为Response调用开始中间件链的 process_response 的流程Request不在调用其他的 process_request 方法，由调度器从新安排下载Noneprocess_exception 会被调用，如果没有此方法，则 request.errback会被调用，此方法也没有，该方法将会被彻底忽略scrapy-redisscrapy-redis 是 scrapy 的一个插件，帮助 scrapy 多个进程之间的协作，scrapy-redis 安装：1pip install scrapy-redis安装好以后，只需要在 settings.py 文件中修改配置：12345678# 调度器SCHEDULER = \"scrapy_redis.scheduler.Scheduler\"# 过滤器DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\"# Redis 的路径REDIS_URL = 'redis://user:pass@hostname:9001'# 退出时清空 redis 数据SCHEDULER_PERSIST = Trueredis 中的key通过 redis 的客户端可以看到如下的 key:keyvaluespiderName:dupefilter这个是一个 set 类型的数据，存储的是所有url，32位md5的哈希值quotes:requests类型为zset，存储 yield的request对象，还没有请求过得。spiderName:items这一个需要开启scrapy_redis 提供的pipelines123ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125;seleniumSelenium 是一个自动化测试框架，在制作爬虫的时候因为反爬机制，无法获取数据，这个时候我们可以使用selenium，它相当于一个真正的浏览器。1pip install selenium要使用这个框架，需要下载对应的浏览器驱动，放在 PATH 路径下。启动一个浏览器123456from selenium import webdriverfirefox = webdriver.Firefox()# 打开京东firefox.get(url)如果想要使用其他的浏览器在 webdriver 创建对应的对象。控制浏览器浏览器创建的对象，提供一系列的方法用来操控浏览器：方法作用get(url)打开一个 urlfind_element_by_{tag}查找一个元素，{tag 的选项有 id、clss、xpahtfind_elements_by_{tag}效果同上，查询多个save_screenshot(file_name)截图，将页面的内容截图下来，保存在一个文件input 表单操作使用浏览器对象查询出 input 对象的时候，可以通过以下方法来操作：方法作用click()如果是链接和按钮可以使用此方法send_keys(key)如果是一个输入框则可以使用此方法发送数据无界面使用无界面的方式来运行 selenium ，效果最终都是一只的，只不过不显示而已：123from selenium.webdriver.firefox.options import Optionsoptions = Options()options.add_argument('--headless')使用不同的浏览器只要替换 webdriver 下的包就可以了。等待隐式等待12# 查找某一个元素如果没有立即找到就等待10秒firefox.implicitly_wait(10)显示等待12345678from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ecfrom selenium.webdriver.common.by import Bysort_btn = WebDriverWait(fierfox, 10).until( ec.presence_of_element_located(By.XPATH), \"//button\")查找一个按钮，最常等待 10 秒，直到找到这个按钮。","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"Python","slug":"Python","permalink":"https://shijiazhuangbaifeng.github.io/tags/Python/"}]},{"title":"MySql 高级","slug":"MySql 高级","date":"2020-07-31T09:23:44.969Z","updated":"2020-08-01T06:18:37.947Z","comments":true,"path":"2020/07/31/MySql 高级/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/07/31/MySql%20%E9%AB%98%E7%BA%A7/","excerpt":"","text":"MySql 高级介绍MYSQL 高级的部分涉及的有：索引优化：在大的数据量查询情况下数据库锁：防止库存超卖数据库函数：在测试环境下，导出大量数据八字口诀：1234567891011全职匹配我最爱，最左前缀要遵守；带头大哥不能死，中间兄弟不能断；索引列上少计算，范围之后全失效；LIKE百分写最右，覆盖索引不写*；不等空值还有OR，索引影响要注意；VAR引号不可丢，SQL优化有诀窍。索引是什么Mysql 官方对索引的定义是：索引 (Index) 是帮助 MySql 高效获取数据的数据结构可以得到索引的本质，索引是数据结构，可以理解：‘排好序的快速查找的数据结构’。索引的分类类型作用单值索引普通索引复合索引多个字段是一个索引唯一索引索引所在字段是惟一的Explain介绍Explain 查看执行计划，可以模拟执行SQL 语句，从而知道 mysql 是如何执行我们的 SQL 语句，将一个 Sql 语句执行过程中， Mysql 引擎将执行参数列出来，分析 SQL 语句和表结构的性能瓶颈，进一步优化 SQL。1EXPLAIN SELECT * FROM tbl_dept返回的内容有：123+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------+作用表的读取顺序数据读取操作的读取数据类型哪些索引可以使用哪些索引被实际使用表之间的引用每个表有多少行被优化器执行返回结果字段作用idSQL 语句在 EXPLAIN 的执行查询的序列号，表示查询中 select 子句或操作表的顺序，ID 越大执行越早，id 相同，从前往后执行。select_type查询语句类型table涉及到的表，可能是实际表和衍生表type显示查询索引使用何种数据类型 :systen &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; allkey实际使用的索引，如果没有使用则为 NULLkey_lenselect_type 的值有：SIMPLE ：单表查询DERIVED：衍生表PRIMARY ：主查询SUBQUERY：子查询UNION UNION RESULT：联合查询","categories":[],"tags":[]},{"title":"Elasticsearch","slug":"Eelasticsearch","date":"2020-07-15T02:53:21.000Z","updated":"2020-07-16T23:41:46.146Z","comments":true,"path":"2020/07/15/Eelasticsearch/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/07/15/Eelasticsearch/","excerpt":"","text":"安装 Elasticsearch单机安装 Elasticsearch安装 Elasticsearch 需要的文件有：analysis-ikcerebro-0.8.3elasticsearch-6.3.1.tar.gzelasticsearch-analysis-ik-5.6.4.zipelasticsearch-analysis-ik6.rarkibana-5.6.4-linux-x86_64.tar.gzkibana-6.3.1-linux-x86_64.tar.gz进入 ubuntu 服务器，创建目录，将以上的文件上传到该文件目录中：1234567891011121314151617mkdir -p /url/local/elasticsearchcd /url/local/elasticsearchls -latotal 400136drwxr-xr-x 5 root root 4096 Jul 12 23:39 ./drwxr-xr-x 12 root root 4096 Jul 12 23:34 ../drwxrwxr-x 3 miao miao 4096 Jul 12 23:32 analysis-ik/drwxrwxr-x 5 miao miao 4096 Jul 12 23:32 cerebro-0.8.3/-rw-rw-r-- 1 miao miao 52402153 Apr 24 2019 cerebro-0.8.3.zip-rw-rw-r-- 1 miao miao 91429350 Apr 12 2019 elasticsearch-6.3.1.tar.gz-rw-rw-r-- 1 miao miao 4502411 Apr 15 2018 elasticsearch-analysis-ik-5.6.4.zip-rw-rw-r-- 1 miao miao 4128393 May 17 2019 elasticsearch-analysis-ik6.rar-rw-rw-r-- 1 miao miao 51834711 Apr 15 2018 kibana-5.6.4-linux-x86_64.tar.gz-rw-rw-r-- 1 miao miao 205397076 Apr 12 2019 kibana-6.3.1-linux-x86_64.tar.gz解压 elasticsearch-6.3.1.tar.gz 进入文件中进行配置：1cd elasticsearch-6.3.1运行 Elasticsearch123./elasticsearchnohup ./elasticsearch &amp;浏览器访问 192.168.159.135:92001234567891011121314151617&#123; \"name\" : \"QNWRcMX\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"0xiTx31VTxy8UUxaRR96AA\", \"version\" : &#123; \"number\" : \"6.3.1\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"eb782d0\", \"build_date\" : \"2018-06-29T21:59:26.107521Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.3.1\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" &#125;, \"tagline\" : \"You Know, for Search\"&#125;以上步骤进行完了以后还需要配置 elasticsearch-6.3.1/config 目录下的两个配置文件： jvm.options 和 elasticsearch.yml 两个配置文件。123# 测试环境使用内存大小-Xms256-Xmx256在 jvm.options 中配置虚拟机的内存大小。123network.host: 192.168.159.135http.port: 9200如果不配置以上内存，elastticsearch 只能够在内网访问。错误can not run elasticsearch as root完整的错误信息如下：1234567891011121314org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-6.3.1.jar:6.3.1] at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-6.3.1.jar:6.3.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[elasticsearch-6.3.1.jar:6.3.1]Caused by: java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:104) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:171) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[elasticsearch-6.3.1.jar:6.3.1] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[elasticsearch-6.3.1.jar:6.3.1] ... 6 more为了安全问题 elasticsearch 不能通过 root 账户直接启动，需要创建一个新的用户：123adduser escd /url/local/elasticsearchmax virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]Linux 默认的最大线程数12345678910111213vim /etc/security/limits.conf* hard nofile 655360* soft nofile 131072* hard nproc 4096* soft nproc 2048vim /etc/sysctl.confvm.max_map_count=655360fs.file-max=655360sysctl -pElasticsearch 集群修改配置文件 elasticsearch.yml123456789101112131415161718192021# 集群名称（相同）cluster.name: aubin-cluster # 集群下节点的名称，节点名称，仅仅是描述名称，用于在日志中区分（自定义）node.name: els1（必须不同）#指定了该节点可能成为 master 节点，还可以是数据节点 node.master: true node.data: true# 数据的默认存放路径（自定义）path.data: /opt/data# 日志的默认存放路径 path.logs: /opt/logs # 当前节点的IP地址 network.host: 192.168.0.1 # 对外提供服务的端口http.port: 9200 #9300为集群服务的端口 transport.tcp.port: 9300# 集群个节点IP地址，也可以使用域名，需要各节点能够解析 discovery.zen.ping.unicast.hosts: [\"172.18.68.11\", \"172.18.68.12\",\"172.18.68.13\"] # 为了避免脑裂，集群节点数最少为 半数+1discovery.zen.minimum_master_nodes: 2注意：清空data和logs数据使用ElasticsearchElasticsearch 交互方式通过 HTTP 协议，以 JSON 格式 RESTFUL API，GET POST PUT DELETE HEADSearch API查询有哪些索引1GET /_cat/indices?v表头的含义：名称含义healthgreen(集群完整) yellow(单点正常、集群不完整) red(单点不正常)status是否能使用index索引uuid索引统一 idpri主节点有几个rep从节点docs.count文档数docs.delete文档删除数store.size整体占空间大小pri.store.size主节点占空间大小增删改查增加一个索引：1PUT index_name添加一个文档12345678910111213141516171819PUT /index_name/type/id&#123; \"filed\":\"value\"&#125;&#123; \"_index\": \"miao\", \"_type\": \"person\", \"_id\": \"1\", \"_version\": 2, \"result\": \"updated\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 1, \"_primary_term\": 1&#125;一个索引中只能够存在一个 type，PUT 既能插入也能更新查询索引的信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859GET /index_name/_search&#123; # 占用空间 \"took\": 74, \"timed_out\": false, # 分片 \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;,# 命中信息 \"hits\": &#123; # 命中数据量 \"total\": 3, # 命中数据最大评分 \"max_score\": 1, \"hits\": [ &#123; \"_index\": \"miao\", \"_type\": \"person\", \"_id\": \"2\", \"_score\": 1, \"_source\": &#123; \"id\": 2, \"name\": \"yindongfa\", \"age\": 19, \"sex\": \"woman\" &#125; &#125;, &#123; \"_index\": \"miao\", \"_type\": \"person\", \"_id\": \"1\", \"_score\": 1, \"_source\": &#123; \"id\": 1, \"name\": \"miaochuntong\", \"age\": 18, \"sex\": \"man\" &#125; &#125;, &#123; \"_index\": \"miao\", \"_type\": \"person\", \"_id\": \"3\", \"_score\": 1, \"_source\": &#123; \"id\": 3, \"name\": \"fanzhengyao\", \"age\": 21, \"sex\": \"man\" &#125; &#125; ] &#125;&#125;定义数据结构es 通过 mapping 来定义数据结构，es 规定一个索引下面只能够有一个类型。1GET /atguigu/_mapping12345678910111213141516171819202122232425262728293031323334353637&#123; \"atguigu\": &#123; #索引名称 \"mappings\": &#123; #mapping设置 \"student\": &#123; #type名称 \"properties\": &#123; #字段属性 \"clazz\": &#123; \"type\": \"text\", #字段类型，字符串默认类型 \"fields\": &#123; #子字段属性设置 \"keyword\": &#123; #分词类型（不分词） \"type\": \"keyword\", \"ignore_above\": 256 &#125; &#125; &#125;, \"description\": &#123; \"type\": \"text\", \"fields\": &#123; \"keyword\": &#123; \"type\": \"keyword\", \"ignore_above\": 256 &#125; &#125; &#125;, \"name\": &#123; \"type\": \"text\", \"fields\": &#123; \"keyword\": &#123; \"type\": \"keyword\", \"ignore_above\": 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;默认数据类型es 规定了，type 的每一个字段的数据类型是什么都由 Mapping 规定，如果没有规定 type ，es 会根据数据类型来推定数据类型。分词器分词是指将文本转换成一系列单词（term or token）的过程，也可以叫做文本分析，在es里面称为Analysis，elasticsearch 的分词器默认是英文分词器，不支持中文，所以需要安装中文分词器：分词器下载地址：https://github.com/medcl/elasticsearch-analysis-ik分词器机制Character Filter对原始文本进行处理例：去除html标签、特殊字符等Tokenizer将原始文本进行分词例：培训机构–&gt;培训，机构Token Filters分词后的关键字进行加工例：转小写、删除语气词、近义词和同义词等安装分词器下载 IK 中文分词器对应的 Elastisearch 版本，解压到 elasticsearch/plugins/ik 目录下。测试分词器ik_smart12345GET _analyze&#123; \"analyzer\": \"ik_smart\", \"text\": \"我的名字是姚明\"&#125;分词结果如下：123456789101112131415161718192021222324252627282930313233343536373839&#123; \"tokens\": [ &#123; \"token\": \"我\", \"start_offset\": 0, \"end_offset\": 1, \"type\": \"CN_CHAR\", \"position\": 0 &#125;, &#123; \"token\": \"的\", \"start_offset\": 1, \"end_offset\": 2, \"type\": \"CN_CHAR\", \"position\": 1 &#125;, &#123; \"token\": \"名字\", \"start_offset\": 2, \"end_offset\": 4, \"type\": \"CN_WORD\", \"position\": 2 &#125;, &#123; \"token\": \"是\", \"start_offset\": 4, \"end_offset\": 5, \"type\": \"CN_CHAR\", \"position\": 3 &#125;, &#123; \"token\": \"姚明\", \"start_offset\": 5, \"end_offset\": 7, \"type\": \"CN_WORD\", \"position\": 4 &#125; ]&#125;ik_max_word12345GET _analyze&#123; \"analyzer\": \"ik_max_word\", \"text\": \"我的名字是姚明\"&#125;分词结果：123456789101112131415161718192021222324252627282930313233343536373839&#123; \"tokens\": [ &#123; \"token\": \"我\", \"start_offset\": 0, \"end_offset\": 1, \"type\": \"CN_CHAR\", \"position\": 0 &#125;, &#123; \"token\": \"的\", \"start_offset\": 1, \"end_offset\": 2, \"type\": \"CN_CHAR\", \"position\": 1 &#125;, &#123; \"token\": \"名字\", \"start_offset\": 2, \"end_offset\": 4, \"type\": \"CN_WORD\", \"position\": 2 &#125;, &#123; \"token\": \"是\", \"start_offset\": 4, \"end_offset\": 5, \"type\": \"CN_CHAR\", \"position\": 3 &#125;, &#123; \"token\": \"姚明\", \"start_offset\": 5, \"end_offset\": 7, \"type\": \"CN_WORD\", \"position\": 4 &#125; ]&#125;相关度算分相关性算分：指文档与查询语句间的相关度，通过倒排索引可以获取与查询语句相匹配的文档列表如何将最符合用户查询需求的文档放到前列呢？本质问题是一个排序的问题，排序的依据是相关性算分，确定倒排索引哪个文档排在前面影响相关度算分的参数：1、TF(Term Frequency)：词频，即单词在文档中出现的次数，词频越高，相关度越高2、Document Frequency(DF)：文档词频，即单词出现的文档数3、IDF(Inverse Document Frequency)：逆向文档词频，与文档词频相反，即1/DF。即单词出现的文档数越少，相关度越高（如果一个单词在文档集出现越少，算为越重要单词）4、Field-length Norm：文档越短，相关度越高Elasticsearch 数据导入","categories":[{"name":"分布式","slug":"分布式","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"全文检索","slug":"全文检索","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}]},{"title":"Pandas","slug":"Pandas","date":"2020-07-13T02:53:21.000Z","updated":"2020-07-23T14:16:54.921Z","comments":true,"path":"2020/07/13/Pandas/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/07/13/Pandas/","excerpt":"","text":"pandas 是使 python 成为强大数据分析的语言原因之一，其中最重要的是：DataFrame：数据表格，二维（行、列）Series：一维1234567import numpy as npimport pandas as pdfrom pandas import Series,DataFrame# 画图、可视化import matplotlib.pylab as plt导入数据分析三剑客。SeriesSeries 是一组类似一位数组的的对象，由以下两部分组成：values：一组数据(ndarray类型)index：相关数据的索引标签Series 的创建由列表或者 ndarray 创建123series = Series(data = np.random.randint(10,100,size=10), index=list('asdfghjklq'), name='Python')seriesdata ： 相当于 valuesname ： 相当于去一个名字通过字典来创建123series = Series(&#123;'a':100, 'b':90, 'c':80&#125;, name='Python')series如果不指定 Series 的索引，默认就会创建整数类型的索引查看 series 的类型：123type(series)pandas.core.series.SeriesSeries 索引普通索引123series['a']series[['a','b']]隐式索引12345series[0]# 默认的索引series.iloc[0]# 和原来一抹一样series.loc['a']基本概念可以将 Series 看做成是有一个有定长度的有序字典，可以通过 shape 、 index 、 values 和 size 得到 series 的属性。123456789101112131415series.shape(10,)series.indexIndex(['a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'q'], dtype='object')series.valuesarray([39, 50, 37, 86, 53, 30, 98, 20, 14, 52])series.size10可以通过 head() 和 tail() 来方便的查看 series 的样式，默认显示前五个和后五个。123series.head()series.tail()使用 isnull 和 notnull 查看是否为空：123series.isnull()series.notnull()Series 的运算适用于 ndarry 的运算同样都适合 seriesDataFrameDataFrame 创建123DataFrame(data=&#123;'Python': np.random.randint(0,150,size=5), 'Math': np.random.randint(0,150,size=5), 'En': np.random.randint(0,150,size=5)&#125;, index=list('abcde'))第二种方式，直接指定，列名：1DataFrame(data=np.random.randint(0,150,size=(10,5)), index=list('asdfgzxcvb'), columns=['Python', 'JAVA', 'C++', 'C', 'GO'])保存与读取保存为 csv 文件1data.to_csv('./data.csv')保存为 excel 文件1data.to_excel('./data.xlsx')保存为 excel 文件，需要安装 openpyxl 文件读取指定的文件123data = pd.read_csv('data.csv')data = data.rename(&#123;'Unnamed: 0':'index'&#125;, axis=1)data.set_index(keys= 'index')DataFrame 的属性index ，返回所有的索引1data.indexcolumns 列的列名1data.columnsshape 数组格式1data.shapeDataFrame 索引通过列进行索引通过字典的方式1data['Python']通过属性的方式1data.Python返回的是一个 Series 对象，所以 DataFrame 是由多个 Series组成，他们共用一个 index通过行索引获取1data.iloc[[1,2]]如果取一个就是 Series切片直接使用切片，不能对列进行切片，但是可以对行：1data['a':'f']列切片1data.iloc[:,1:3]DataFrame 的运算相关度计算1data.corr()方差1data.var()DataFrame 的基本信息1data.info()DataFrame 之间的运算dataFrame.add() 两者相加1data_class_01 + data_class_02数据清洗最常见的无效数据是 np.NaN 和 None，针对这些数据我们需要找到，然后进行清洗。数据级联DataFrame 的数据级联方式和 NdArray 几乎完全一样。简单拼接行与行之间的拼接，也是最简单的拼接：1234567df1 = DataFrame(data = np.random.randint(0,150,size=(5,3)), index=list('ABCDE'), columns=['Python','C++','JAVA'])df2 = DataFrame(data = np.random.randint(0,150,size=(5,3)), index=list('ABCDE'), columns=['Python','C++','JAVA'])display(df1,df2)pd.concat([df1, df2])通过 axis 的值，来设值列之间的拼接123df3 = DataFrame(data = np.random.randint(0,150,size=(5,2)), index=list('ABCDE'), columns=['C#','C'])pd.concat([df1,df2], axis=1)joinconcat 函数有 join 参数，作用：inner 两者共同索引才会合并outher合并所有ignore_index 表示是否忽略行索引，重新制定，从 0 开始。合并merge 和 concat 不同的是， merge 会根据相同的属性值进行组合。一对一合并123456789df1 = DataFrame(data = &#123;'id':[1,2,3,4,5], 'name':['王五','咋说','zhaoliu','曾小贤','先把宝宝'],'age':[18,58,12,34,25]&#125;)df1df2 = DataFrame(data = &#123;'id':[1,3,5], 'salary' : [1500,2500,3000]&#125;)df2df1.merge(df2)多对一12345df3 = DataFrame(data = &#123;'id':[1,1,1], 'salary' : [1500,2500,3000]&#125;)df3df1.merge(df3)on当合并的的列，两个 DataFrame 的列不同可以调整 left_on 和 right_on内合并和外合并参数 how 的值可以有：outher 无论对不对其都保留inner 只保留对齐的left 保留左边的right 保留右边的多层索引多层索引就是在一层索引下还可以有索引，最方便的创建方法是给 index 参数，传递两个或更多的数组。SeriesSeries 也可以创建多层索引：12# 双层索引Series(np.random.randint(0, 150, size=6), index=pd.MultiIndex.from_product([list('ABC'),['期末','期中']]))显示构造 pd.MultiIndex使用数组1234Series(np.random.randint(0, 150, size=4), index=pd.MultiIndex.from_arrays( [list('AABB'),['期末','期中','期末','期中']] ) )使用数组，需要将每一层的索引都准备好。使用 product1234Series(np.random.randint(0, 150, size=15), index=pd.MultiIndex.from_product([list('ABCDE'), ['Python','C++','C']]) )DataFrameDataFrame 和 Series 创建多层索引的方式一样。多层索引既可以是行，也可以是列12DataFrame(np.random.randint(0,150,size=(5,6)), index=list('ABCDE'), columns=pd.MultiIndex.from_product([['Python', 'C++', 'C'], ['期末','期中']]))取值字典1data['Python']属性1data.Python调用行（iloc），返回一行1data.iloc[0]数据聚合","categories":[{"name":"Python","slug":"Python","permalink":"https://shijiazhuangbaifeng.github.io/categories/Python/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"python第三方库","slug":"python第三方库","permalink":"https://shijiazhuangbaifeng.github.io/tags/python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"OpenCV","slug":"OpenCV","date":"2020-07-13T02:53:21.000Z","updated":"2020-07-15T05:09:25.336Z","comments":true,"path":"2020/07/13/OpenCV/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/07/13/OpenCV/","excerpt":"","text":"","categories":[{"name":"图像识别","slug":"图像识别","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"python第三方库","slug":"python第三方库","permalink":"https://shijiazhuangbaifeng.github.io/tags/python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"Numpy","slug":"Numpy","date":"2020-07-08T02:53:21.000Z","updated":"2020-07-10T14:09:00.599Z","comments":true,"path":"2020/07/08/Numpy/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/07/08/Numpy/","excerpt":"","text":"NumPy是Python中科学计算的基础包。它是一个Python库，提供多维数组对象，各种派生对象（如掩码数组和矩阵），以及用于数组快速操作的各种API，有包括数学、逻辑、形状操作、排序、选择、输入输出、离散傅立叶变换、基本线性代数，基本统计运算和随机模拟等等。1pip install numpy创建数组array根据已有的 array 对象，来创建经过 numpy 增强以后的 ndarray 对象：1234567import numpy as npl = &#123;1, 2, 3, 4, 5, 6, 7&#125;l = np.array(l)type(l)生成数组ones生成一个ndarray 数组的元素全部为 1：1np.ones(shape=(5,5), dtype=np.int8)shape 表示生成数组形状，例如上面的生成二维的数组，第一维和第二维都分别有 5 的元素，dtype 表示元素类型。zeros1np.zeros(shape=(2,5,2), dtype=np.float)效果同上，生成元素为 0 的数组。full1np.full(shape=(3,1,4), fill_value=3.14)自定义填充数组元素的值。eye1np.eye(N=5)生成单位矩阵。linespace1np.linspace(0,100,101)左闭右闭，等差数列，上一个的差和下一个的差永远相等。arange1np.arange(0,100,3)这个函数和 range 函数效果几乎完全相同。正太分布1np.random.randn(4,5)平均值为 0 ，方差是 1，这个函数是固定的。1n = np.random.normal(loc=175,scale=2,size=100).round(2)normal 可以指定 平均值和方差，round函数表示保留的小数位。数据方法round1n = np.random.normal(loc=175,scale=2,size=100).round(2)指定数据元素保留的小数位。三角函数np.sin1np.sin(n)对 n 的每一个元素进行 sin 操作。ndarrayndarray属性属性/方法作用shape数组对象的格式dtype数组元素类型ndim数组维度size数组元素个数nbytes数组元素消耗的总字节数reshape(shape[, order])返回包含具有新形状的相同数据的数组。切片操作","categories":[{"name":"Python","slug":"Python","permalink":"https://shijiazhuangbaifeng.github.io/categories/Python/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"python第三方库","slug":"python第三方库","permalink":"https://shijiazhuangbaifeng.github.io/tags/python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/"}]},{"title":"Matplotlib","slug":"matplotlib","date":"2020-07-08T02:53:21.000Z","updated":"2020-07-08T04:00:12.015Z","comments":true,"path":"2020/07/08/matplotlib/","link":"","permalink":"https://shijiazhuangbaifeng.github.io/2020/07/08/matplotlib/","excerpt":"","text":"Matplotlib用来绘制图表的工具，数据可视化，能够绘制柱状图，直方图，折线图等图表，安装Matplotlib:1pip install matplotlib绘制折线图使用Matplotlib绘图要使用pyplot这个模块：1import matplotlib.pyplot as pyplotpyplot关于图表的基本方法如下：方法参数作用pyplot.title()label：图表的名称当前图表的名称pyplot.rcParams[&#39;font.sans-serif&#39;]赋予一个字符串，字体设置字体，对中文进行支持pyplot.xlabel()xlabel：x轴的名称设置x轴名称pyplot.ylabel()ylabel：y轴的名称设置y轴名称pyplot.show()将内存的表展示出来，重新绘制pyplot.savefig()第一个参数为名称保存为图片12345678910111213141516171819202122232425import matplotlib.pyplot as pylot# title 图标的标题pylot.title('折线图')# 中文标题pylot.rcParams['font.sans-serif'] = 'Kaiti'# 直线# pylot.plot((1,4),(2,8))# 坐标x = [1, 2, 3, 4, 5, 5]y = [1, 2, 3, 4, 5, 4]# 折线 linewidth 宽度pylot.plot(x, y, linewidth=2)# x 轴名称pylot.xlabel('x轴')# y 轴的名称pylot.ylabel('y轴')# 显示pylot.show()绘制一元二次方程1234567891011121314151617181920212223242526import matplotlib.pyplot as pylotimport numpy as np# x 和 y 的值x = np.linspace(-10, 10, 100, dtype=float)y = x ** -1print(x)print(y)# pylot.axhline(0)pylot.axvline(0, color='red')# 折线 linewidth 宽度pylot.plot(x, y, linewidth=1)# x 轴名称pylot.xlabel('x')# y 轴的名称pylot.ylabel('y')# 保存图片pylot.savefig(\"一元二次方程\")# 显示pylot.show()分区sublot将画布分为n行n列，并进入某一块分区：12# 分区pylot.subplot(2,2,1)想要切换分区再次调用即可，但是行列要和上一次一致散点图散点图使用的是scatter方法，该方法有以下参数：参数作用x一个集合，存放xy一个集合，存放ys大小，一个集合，要和点的个数一致c颜色，和s相同alpha透明度0~1123456789101112131415import numpy as npimport matplotlib.pyplot as ploy# 等差数列x = np.random.randint(10,100,50)y = np.random.randint(10,100,50)# sizesize = np.random.randint(50,500,50)# colorcolor = np.random.randint(10,100,50)# 绘制散点图ploy.scatter(x,y,s=size,alpha=0.5,c=color)# 展示ploy.show()图例在绘制折线图的时候，会绘制许多条线，每一条线使用不同的样式来描述，需要一块内容来描述每一条线的作用：12345678910111213141516171819import numpy as npimport matplotlib.pyplot as pylot# 生成 xx = np.linspace(-10, 10, 100)sin_y = np.sin(x)cos_y = np.cos(x)# 绘制pylot.plot(x, sin_y, '--y', label='sin')pylot.plot(x, cos_y, '-g', label='cos')pylot.axhline(0, c=\"#FF6347\", ls=\"--\", lw=1)# 显示图例 loc : 图例的位置 labels : 每一个线条的注释 lines : 每一个线条的样式pylot.legend(loc='upper left') # fancybox边框 framealpha透明度 shadow明影边框宽度# 展示pylot.show()plot的label参数就是图例的信息，legend显示图例，默认是不显示的，接收一个参数，图例所在的位置.格式化在plot函数中还接受一个参数fmt格式化线条：color字符颜色字符颜色字符颜色字符颜色b蓝色r红色m洋红色k黑色g绿色c青色y黄色w白色marker字符标记字符标记字符标记字符标记.点标记&gt;右三角标记p五边形标记D菱形标记,像素标记1三叉戟标记*星形标记d菱形标记o圆圈标记2三叉戟标记h六角形标记v倒三角标记3三叉戟标记H六角形标记_横线标记^正三角标记4三叉戟标记++标记&lt;左三角标记s正方形标记xx标记&lt;line字符格式字符格式字符格式字符格式-实线–虚线-.点划线:点线柱状图绘制柱状图使用的函数的bar和barh，barh是用来绘制水平方向的柱状图：12345678910111213141516171819import matplotlib.pyplot as pyplotimport numpy as np# 年份x = ['1980', '1981', '1982']# 销量y = [2000, 3000, 4000]# 绘制柱状图pyplot.bar(x, y, width=0.5) # width 在原来的宽度倍数# 水平方向绘制pyplot.barh(x, y)# x 坐标的值# yplot.xticks()# 展示pyplot.show()xticks用来修改x坐标上x的值电影票销量12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as pyplot# 中文标题pyplot.rcParams['font.sans-serif'] = 'Kaiti'# 电影movie = ['哪吒之魔童降世', '流浪地球', '复仇者联盟4：终局之战', '我和我的祖国']# 三天内票房数movie_day1 = np.random.randint(1000, 2000, size=len(movie))movie_day2 = np.random.randint(1000, 2000, size=len(movie))movie_day3 = np.random.randint(1000, 2000, size=len(movie))print(movie_day1)# x 坐标x = np.arange(len(movie))# 绘制柱状图pyplot.bar(x, movie_day1, alpha=0.5 , width=0.3, label='第一天')pyplot.bar(x + 0.3, movie_day2, alpha=0.5, width=0.3, label='第一天')pyplot.bar(x + 0.6, movie_day3, alpha=0.5, width=0.3, label='第一天')# 设置值pyplot.xticks(x + 0.3,movie)pyplot.legend()# 展示pyplot.show()饼状图饼状图用到的方法是pie：1234567891011121314151617import numpy as npimport matplotlib.pyplot as pyplotman = 1060woman = 1520# 中文标题pyplot.rcParams['font.sans-serif'] = 'Kaiti'# 比例man_perc = man / (man + woman)woman_perc = 1 - man_perc# 绘制饼状图pyplot.pie([man_perc,woman_perc],labels=['男','女'],colors=['red','green'])# 展示pyplot.show()直方图直方图用来观测数据的分布情况：123456789101112131415161718\"\"\"直方图关心的是分布的状态\"\"\"import numpy as npimport matplotlib.pyplot as pyplot# 正太分布random = np.random.normal(0,0.8,1000)random1 = np.random.normal(1,0.8,1000)random2 = np.random.normal(2,0.8,1000)print(random)# 绘制 binds = 直方图的数量pyplot.hist(random,bins=100,alpha=0.5)pyplot.hist(random1,bins=100,alpha=0.5)pyplot.hist(random2,bins=100,alpha=0.5)pyplot.show()","categories":[{"name":"Python","slug":"Python","permalink":"https://shijiazhuangbaifeng.github.io/categories/Python/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"python第三方库","slug":"python第三方库","permalink":"https://shijiazhuangbaifeng.github.io/tags/python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}]}],"categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://shijiazhuangbaifeng.github.io/categories/JAVA/"},{"name":"爬虫","slug":"爬虫","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E7%88%AC%E8%99%AB/"},{"name":"分布式","slug":"分布式","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Python","slug":"Python","permalink":"https://shijiazhuangbaifeng.github.io/categories/Python/"},{"name":"图像识别","slug":"图像识别","permalink":"https://shijiazhuangbaifeng.github.io/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://shijiazhuangbaifeng.github.io/tags/JAVA/"},{"name":"原理","slug":"原理","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E5%8E%9F%E7%90%86/"},{"name":"代理","slug":"代理","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E4%BB%A3%E7%90%86/"},{"name":"爬虫","slug":"爬虫","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"Python","slug":"Python","permalink":"https://shijiazhuangbaifeng.github.io/tags/Python/"},{"name":"分布式","slug":"分布式","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"全文检索","slug":"全文检索","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"},{"name":"数据分析","slug":"数据分析","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"python第三方库","slug":"python第三方库","permalink":"https://shijiazhuangbaifeng.github.io/tags/python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://shijiazhuangbaifeng.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}]}